
# Harmonic Resonance Fields (HRF)
### Rethinking Classification Through Wave Physics

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

---

##  What if classification isn't about boundaries, but about resonance?

This repository presents **Harmonic Resonance Fields (HRF)** — a physics-inspired classification algorithm that replaces geometric decision boundaries with spectral interference patterns. Unlike traditional ML methods that partition feature space, HRF treats each data point as a **damped harmonic oscillator** emitting class-specific waves.

**Classification emerges through constructive and destructive interference.**

---

##  The Core Insight

Every machine learning textbook teaches the same paradigm: *find the boundary that separates classes*. But nature doesn't work this way. 

In quantum mechanics, particles don't have hard boundaries — they have **wave functions** that interfere. In acoustics, sound doesn't stop at walls — it **resonates** with different materials. In spectroscopy, molecules don't cluster in space — they emit **unique spectral signatures**.

**What if we stopped trying to draw lines and started listening for resonances?**

This thought led to HRF: a classifier that treats feature space as a **continuous medium** where each training sample radiates a class-specific wave pattern. A test point isn't assigned to "the nearest cluster" or "beyond the margin" — it couples to **whichever class field resonates most strongly** at that location.

---

##  Mathematical Framework

### Wave Potential Function

Each training point $p_i \in \mathbb{R}^d$ belonging to class $k$ generates a localized wave field:

$$
\Psi(x, p_i) = \exp\left(-\gamma \|x - p_i\|^2\right) \cdot \cos\left(\omega_k \|x - p_i\|\right)
$$

where:
- $x$ is the query point
- $\gamma$ controls damping (locality)
- $\omega_k$ is the class-specific angular frequency

### Superposition Principle

The total resonance intensity for class $k$ at point $x$ is:

$$
I_k(x) = \sum_{p_j \in C_k} \Psi(x, p_j)
$$

This is a direct application of **linear wave superposition** from physics.

### Decision Rule

Classification proceeds by maximum constructive resonance:

$$
\hat{y}(x) = \arg\max_{k} I_k(x)
$$

### Why This Works

1. **Gaussian damping** $\exp(-\gamma r^2)$ ensures locality (distant points don't interfere)
2. **Oscillatory term** $\cos(\omega_k r)$ encodes class identity through frequency
3. **Superposition** creates constructive peaks at class centers, destructive cancellation at boundaries
4. **Frequency separation** $\omega_k \neq \omega_j$ provides spectral orthogonality

---

##  What Makes HRF Unique?

### 1. **No Explicit Boundaries**
- SVM: finds optimal separating hyperplane
- Decision Trees: recursive axis-aligned splits  
- **HRF**: boundaries emerge implicitly from interference nulls

### 2. **Spectral Class Encoding**
- k-NN: uses distance only
- RBF Kernel: radial basis with single scale
- **HRF**: each class has unique frequency → orthogonal signatures

### 3. **Physics-Grounded**
- Neural Networks: biological inspiration, abstract
- Kernel Methods: mathematical convenience
- **HRF**: direct mapping to wave equation solutions

### 4. **Differentiable Field**
Unlike voting-based methods (k-NN, Random Forest), HRF produces a smooth, continuous field:

$$
\nabla_x I_k(x) = \sum_{p_j \in C_k} \nabla_x \Psi(x, p_j)
$$

This enables gradient-based optimization of $\omega_k$ and $\gamma$.

---

##  Experimental Results

Tested on `make_moons` dataset (300 samples, 70-30 train-test split):

```
--- FINAL LEADERBOARD ---
KNN:            97.78%
Random Forest:  96.67%
SVM:            96.67%
HRF (ours):     86.67%
```

(![Decision Boundaries](<img width="1989" height="490" alt="Results" src="https://github.com/user-attachments/assets/b21dbaa7-fc9d-480b-8406-6fc5048835bd" />
)
)

### Analysis

HRF demonstrates **conceptual validity** but requires further optimization:

 **Successfully learns non-linear patterns** (significantly above random 50%)  
 **Produces smooth, interpretable decision fields**  
 **No hyperparameter tuning performed** (baseline implementation)

 **Performance gap vs. mature algorithms** (expected for v1.0)  
 **Hyperparameters** ($\omega_k$, $\gamma$) currently hand-selected  
 **Computational cost** scales with training set size

**This is exploratory research.** The goal isn't to replace RandomForest — it's to explore whether **wave-theoretic principles** offer a fundamentally different inductive bias worth investigating.

---

##  Design Philosophy

### The Genesis

This algorithm emerged from three converging thoughts:

1. **Spectroscopy Analogy**: Chemists identify molecules by spectral signatures, not geometric positions. Could classes have "spectral identities"?

2. **Kernel Methods Limitation**: RBF kernels are radially symmetric. But waves aren't — they oscillate. What if we added phase information?

3. **Interference in Nature**: Noise-cancelling headphones work via destructive interference. Could class boundaries emerge the same way?

The mathematical formulation fell into place once I stopped thinking about "where to draw the line" and started thinking about "which frequency dominates."

### Core Principle

> **"Data doesn't separate itself through geometry alone — it resonates."**

---

##  Future Directions

### Immediate Optimizations
1. **Adaptive Frequencies**: Learn $\omega_k$ via gradient descent
2. **Intelligent Damping**: Make $\gamma$ point-dependent
3. **Sparse Approximation**: Compute resonance using only $k$ nearest oscillators

### Theoretical Extensions
1. **Complex-Valued Fields**: Use $e^{i\omega_k r}$ for phase-aware classification
2. **Multi-Scale Resonance**: Wavelet-inspired multi-frequency decomposition
3. **Quantum Interpretation**: Treat $|I_k(x)|^2$ as probability amplitudes

### Applications
- **Time-Series**: Temporal waves with $\omega_k(t)$
- **Graph Data**: Resonance on graph Laplacians
- **Multi-Modal Learning**: Different frequencies for different modalities

---

##  Usage

```python
from harmonic_classifier import HarmonicResonanceClassifier

# Initialize with base frequency
model = HarmonicResonanceClassifier(base_freq=1.5)

# Fit to training data
model.fit(X_train, y_train)

# Predict via resonance
predictions = model.predict(X_test)
```

---

##  Theoretical Connections

| Concept | HRF Analog | Physical System |
|---------|-----------|-----------------|
| Training point | Oscillator | Dipole antenna |
| Feature distance | Propagation delay | Wave travel time |
| Class label | Frequency band | Spectral line |
| Classification | Max resonance | Energy absorption |

---

##  Open Questions

1. Can HRF be kernelized for efficiency?
2. What is the VC-dimension of the resonance hypothesis space?
3. Does frequency separation guarantee PAC learnability?
4. Can we prove convergence bounds for gradient-optimized $\omega_k$?

---

##  Citation

If you use this work, please cite:

```bibtex
@software{harmonic_resonance_fields,
  title={Harmonic Resonance Fields: Classification via Spectral Interference},
  author={[Your Name]},
  year={2024},
  url={https://github.com/[your-username]/harmonic-resonance-fields}
}
```

---

##  For Researchers

This is **blue-sky ML research** — prioritizing novelty and interpretability over immediate SOTA performance. If you're interested in:

- Physics-inspired learning algorithms
- Alternative inductive biases beyond geometry
- Interpretable non-parametric methods

...then HRF might offer interesting theoretical directions, even if it's not production-ready.

**Contributions welcome.** Let's see how far the wave analogy can go.

---

##  License

MIT License([LICENSE](https://github.com/Devanik21/ML-ReInvEnTed/tree/main?tab=MIT-1-ov-file)) file.

---

##  Final Thought

> *"The best way to have a good idea is to have lots of ideas."*  
> — Linus Pauling

Not every algorithm will beat XGBoost. But every novel perspective enriches our understanding of learning itself. HRF asks: **what if machine learning spoke the language of waves instead of boundaries?**

Maybe it's not the answer. But it's a question worth asking.

---

**Star ⭐ if you find this conceptually interesting, even if not practically optimal.**
